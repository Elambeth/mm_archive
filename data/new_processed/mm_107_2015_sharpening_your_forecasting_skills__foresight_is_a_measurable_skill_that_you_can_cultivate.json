{
  "filename": "mm_107_2015_sharpening_your_forecasting_skills__foresight_is_a_measurable_skill_that_you_can_cultivate.pdf",
  "metadata": {
    "format": "PDF 1.7",
    "title": "",
    "author": "",
    "subject": "",
    "keywords": "",
    "creator": "",
    "producer": "www.ilovepdf.com",
    "creationDate": "D:20240423225642-07'00'",
    "modDate": "D:20240423225738-07'00'",
    "trapped": "",
    "encryption": null,
    "csv_data": {
      "id": 107,
      "year": "2015",
      "date": "9/28",
      "institution": "Credit Suisse",
      "title": "Sharpening Your Forecasting Skills: Foresight Is a Measurable Skill That You Can Cultivate",
      "url": "https://12mv2.files.wordpress.com/2024/04/15-09-28-sharpening-your-forecasting-skills__foresight-is-a-measurable-skill-that-you-can-cultivate.pdf"
    },
    "tags": [
      "Decision making",
      "Behavioral finance",
      "Skill vs luck",
      "Quantitative methods"
    ],
    "tagged_at": "2025-01-20T21:28:28.015937"
  },
  "pages": [
    {
      "number": 1,
      "text": " \n \nFOR DISCLOSURES AND OTHER IMPORTANT INFORMATION, PLEASE REFER TO THE BACK OF THIS REPORT. \nSeptember 28, 2015 \n \nAuthors \nMichael J. Mauboussin \nmichael.mauboussin@credit-suisse.com \nDan Callahan, CFA \ndaniel.callahan@credit-suisse.com \n \n \n\u201cBeliefs are hypotheses to be tested, not treasures to be protected.\u201d \nPhilip E. Tetlock and Dan Gardner1 \n \uf402\n Philip Tetlock\u2019s study of hundreds of experts making thousands of \npredictions over two decades found that the average prediction was \u201clittle \nbetter than guessing.\u201d That\u2019s the bad news. \uf402\n Tetlock, along with his colleagues, participated in a forecasting \ntournament sponsored by the U.S. intelligence community. That work \nidentified \u201csuperforecasters,\u201d people who consistently make superior \npredictions. That\u2019s the good news. \uf402\n The key to superforecasters is how they think. They are actively open-\nminded, intellectually humble, numerate, thoughtful updaters, and hard \nworking.  \uf402\n Superforecasters achieve better results when they are part of a team. \nBut since there are pros and cons to working in teams, training is \nessential. \uf402\n Instruction in methods to reduce bias in forecasts improves outcomes. \nThere must be a close link between training and implementation. \uf402\n The best leaders recognize that proper, even bold, action requires good \nthinking. \n \n \nGLOBAL FINANCIAL STRATEGIES \nwww.credit-suisse.com \nSharpening Your Forecasting Skills \nForesight Is a Measurable Skill That You Can Cultivate \n"
    },
    {
      "number": 2,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n2 \nIntroduction: The Bad News and the Good News \n \nWhat if you had the opportunity to learn how to improve the quality of your forecasts, measured as the \ndistance between forecasts and outcomes, by 60 percent? Interested? Superforecasting: The Art and Science \nof Prediction by Philip Tetlock and Dan Gardner is a book that shows how a small number of \n\u201csuperforecasters\u201d achieved that level of skill. If you are in the forecasting business\u2014which is likely if you\u2019re \nreading this\u2014you should take a moment to buy it now. You\u2019ll find that it\u2019s a rare book that is both grounded in \nscience and highly practical.  \n \nPhil Tetlock is a professor of psychology and political science at the University of Pennsylvania who has spent \ndecades studying the predictions of experts. Specifically, he enticed 284 experts to make more than 27,000 \npredictions on political, social, and economic outcomes over a 21-year span ended in 2004. The period \nincluded six presidential elections and three wars. These forecasters had crack credentials, including more \nthan a dozen years of relevant work experience and lots of advanced degrees\u2014nearly all had postgraduate \ntraining and half had PhDs.  \n \nTetlock then did something very unusual. He kept track of their predictions. The results, summarized in his \nbook Expert Political Judgment, were not encouraging.2 The predictions of the average expert were \u201clittle \nbetter than guessing,\u201d which is a polite way to say that \u201cthey were roughly as accurate as a dart-throwing \nchimpanzee.\u201d When confronted with the evidence of their futility, the experts did what the rest of us do: they \nput up their psychological defense shields. They noted that they almost called it right, or that their prediction \ncarried so much weight that it affected the outcome, or that they were correct about the prediction but simply \noff on timing. Overall, Tetlock\u2019s results provide lethal ammunition for those who debunk the value of experts.  \n \nBelow the headline of expert ineffectiveness were some more subtle findings. One was an inverse correlation \nbetween fame and accuracy. While famous experts had among the worst records of prediction, they \ndemonstrated \u201cskill at telling a compelling story.\u201d To gain fame it helps to tell \u201ctight, simple, clear stories that \ngrab and hold audiences.\u201d These pundits are often wrong but never in doubt. \n \nAnother result, which is related to the first, was that what mattered in the quality of predictions was less what \nthe expert thought and more how he or she thought. Tetlock categorized his experts as foxes or hedgehogs \nbased on a famous essay on thinking styles by the philosopher Isaiah Berlin. Foxes know a little about a lot of \nthings, and hedgehogs know one big thing. Foxes did better than the dart-throwing chimp, and hedgehogs did \nworse.  \n \nIt\u2019s not hard to see the link between these findings. Most topics of interest in the economic, social, and \npolitical realms defy tight, simple, and clear stories. But imagine you are the producer of a television show that \ncovers politics. Who do you want to put on the air, the equivocal guest who constantly says \u201con the other \nhand,\u201d or the one who confidently tells a crisp and controversial story? It\u2019s not a hard decision, which is why \nmany hedgehogs are both famous and poor predictors. \n \nWhile the conclusions of Expert Political Judgment were nuanced, they were on balance bad news for pundits. \nDespite how some read his results, Tetlock never believed in the extreme point of view that forecasts are \nuseless. That foxes were better forecasters than the average of all experts provided a strong clue that \nforesight might be a real skill that could be identified and cultivated. Tetlock marked himself as an \u201coptimistic \nskeptic.\u201d   \n \n"
    },
    {
      "number": 3,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n3 \nExpert Political Judgment is excellent scholarly research but is written in, well, scholarly prose. In \nSuperforecasting, Tetlock collaborates with Dan Gardner, a journalist and author of a book about the failure of \nprediction. The result is great research that is easy to read.  \n \nNaturally, Tetlock is not the only one interested in learning how to make effective forecasts. The United States \nintelligence community was also keen to improve the quality of predictions, especially in the wake of the failure \nto anticipate the terrorist acts on September 11, 2001 and the overestimation of the probability of the \nexistence of weapons of mass destruction in Iraq in 2003. An agency within the community, Intelligence \nAdvanced Research Projects Activity (IARPA), was assembled to pursue high-risk research into how to \nimprove American intelligence. IARPA decided to create a forecasting tournament to see if there might be a \nway to sharpen forecasts. \n \nTetlock and some colleagues launched the Good Judgment Project (GJP), one of five scientific teams that \nwould compete to answer questions accurately. The teams could use whatever approaches they wanted to \ngenerate the best possible answers. Starting in September 2011, IARPA asked nearly 500 questions about \nvarious political and economic outcomes. The tournament garnered more than one million individual forecasts \nin the following four years. It is important to note that the time frames for the questions in the IARPA \ntournament, generally one month to one year, were shorter than the three to five years that were common in \nTetlock\u2019s study of experts. \n \nNow the good news: the GJP results beat the control group by 60 percent in year one. Results in year two \nwere even better, trouncing the control group by almost 80 percent. In fact, the GJP did so well that IARPA \ndropped the other teams.  \n \nOf the 2,800 GJP volunteers in the first year of the tournament, the top 2 percent were called \n\u201csuperforecasters.\u201d To give you some sense of their acuity, the superforecasters performed about 30 percent \nbetter than the average for the intelligence community\u2014people who had access to classified data\u2014according \nto an editor at the Washington Post.3  \n \nEncouraged by the GJP\u2019s results, Tetlock came to a couple conclusions. The first is that foresight is a real \nand measurable skill. One test of skill is persistence. High persistence means that you do consistently well \nover time and are not a one-hit wonder. About 70 percent of superforecasters remain in those elite ranks from \none year to the next, vastly more than what chance would dictate. \n \nThe second is that foresight \u201cis the product of particular ways of thinking, of gathering information, of updating \nbeliefs.\u201d Importantly, the essential ingredients of being a superforecaster can be learned and cultivated. The \nbeauty of the GJP is that it was carried out with scientific rigor, which allowed the researchers to distill the \nelements of success. We explore these elements in this report. \n \nEven though most people can improve their thinking and forecasting, there has always been resistance to \nchange based on what Tetlock and Gardner call \u201cillusions of knowledge.\u201d Intuition is one example. Intuition is a \nform of pattern recognition that works in settings with lots of \u201cvalid cues.\u201d4 But intuition is notoriously unreliable \nin unstable or nonlinear environments. An overreliance on intuition leads to poor decisions. \n \nAnother case is insufficient self-reflection. This is in part prompted by a module in our brain that seeks to \nrapidly close cause-and-effect loops. We show you an outcome and your mind quickly comes up with an \nexplanation for it. As Tetlock and Gardner write, \u201cwe move too fast from confusion and uncertainty to a clear \nand confident conclusion without spending any time in between.\u201d This is related to the concept that Daniel \nKahneman, an eminent psychologist, calls thinking fast.5    \n"
    },
    {
      "number": 4,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n4 \nKeeping track of forecasts and the outcomes may not serve an expert\u2019s interests. If you are paid well to be a \npundit, introducing a scoring system offers little upside and lots of downside. In extreme cases, experts are so \nsure that they are correct that they see no need to measure outcomes at all.  \n \nTetlock and Gardner share the words of Galen, the physician to Roman emperors, who practiced in the \nsecond century. Of a particular cure he wrote, \u201cAll who drink of this treatment recover in a short time, except \nthose whom it does not help, who all die. It is obvious, therefore, that it fails only in incurable cases.\u201d \nFortunately, medical researchers have applied the scientific method more rigorously in recent centuries, but \nit\u2019s still easy to spot an overconfident, and unchecked, expert.   \n \nSo what is the source of good forecasting? Tetlock and his colleagues found four drivers behind the success \nof the superforecasters:6 \n \uf402\n Find the right people. You get a 10-15 percent boost from screening forecasters on fluid intelligence \nand active open-mindedness. \n \uf402\n Manage interaction. You get a 10-20 percent enhancement by allowing the forecasters to work \ncollaboratively in teams or competitively in prediction markets.  \n \uf402\n Train effectively. Cognitive debiasing exercises lift results by 10 percent.  \n \uf402\n Overweight elite forecasters or extremize estimates. Results improve by 15-30 percent if you give \nmore weight to better forecasters and make forecasts more extreme to compensate for the conservatism \nof forecasts.  \n \nThe scientists measure these improvements using a Brier score (the appendix provides more detail on the \ncalculation). A Brier score reflects the difference between a forecast and the outcome. Like golf scores, lower \nis better. There are a couple of ways to calculate Brier scores, but a common scale runs from zero to 2.0. \nZero means that the forecast is spot on, 0.50 is a random forecast, and 2.0 means that the forecast is \ncompletely wrong. \n \nBy this scoring, a person who predicts a 55 percent probability of an outcome that happens receives a Brier \nscore of 0.405. A subsequent forecast of a 65 percent probability of an event that occurs gets a Brier score \nof 0.245, nearly a 40 percent improvement.    \n \nThere are a lot of details in finding the right people, effectively building and managing teams, and proper \ntraining. But Tetlock and Gardner offer a simple formula that is at the core of the whole process: \u201cForecast, \nmeasure, and revise: it is the surest path to seeing better.\u201d  \n \nHow can we all become more like superforecasters? To answer that question, we discuss the profile of a \nsuperforecaster, the tools you will need to think like a superforecaster, the lessons for leadership, and the \nvalid doubts that remain. \n \nFind the Right People \n \nThroughout the book, Tetlock and Gardner provide the pieces that allow you to construct the profile of a \nsuperforecaster. Because the GJP researchers took the time to run the forecasters through a battery of \npsychological tests, they were able to examine the personalities of the superforecasters. Further, the data \n"
    },
    {
      "number": 5,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n5 \nallow the researchers to avoid the error of first observing success and then attempting to find common \nattributes after the fact.7 The portrait of a modal superforecaster has four elements: \n  \nPhilosophical Outlook. Superforecasters tend to be comfortable with a sense of doubt. Scientists \nsometimes sense that they know the truth.  Good thinkers can feel the same way. \u201cBut they know they must \nset that feeling aside and replace it with finely measured degrees of doubt,\u201d write Tetlock and Gardner, \u201c\u2014\ndoubt that can be reduced (although never to zero) by better evidence from better studies.\u201d  \n \nRecall that our minds are keen to assign causality. We want the case to be closed. But as Daniel Kahneman \nsays, \u201cIt is wise to take admissions of uncertainty seriously, but declarations of high confidence mainly tell you \nthat an individual has constructed a coherent story in his mind, not necessarily that the story is true.\u201d8     \n \nSuperforecasters are also humble, but not in the sense of feeling unworthy. Rather, their humility comes from \nthe recognition that reality is profoundly complex. Indeed, it is possible to think highly of yourself and to be \nintellectually humble at the same time. Tetlock and Gardner note that, \u201cIntellectual humility compels the careful \nreflection necessary for good judgment; confidence in one\u2019s abilities inspires determined action.\u201d \n \nIt is common, and often soothing, to attribute outcomes to fate. Superforecasters aren\u2019t big believers in fate. \nOn a one to nine \u201cfate score,\u201d where one is a total rejection of fate and nine is complete belief in it, the \naverage adult American falls near the middle. The mean score for a student at the University of Pennsylvania \nis a little lower, the regular forecasters are below that, and the superforecasters are the lowest of these \ngroups. Superforecasters don\u2019t think that what happened had to happen.  \n \nAbility and Thinking Style. The first point is that superforecasters are not geniuses. The researchers tested \nthe fluid and crystallized intelligence of all the GJP volunteers. Fluid intelligence is the ability to think logically \nand to solve novel problems. It doesn\u2019t rely on accumulated knowledge. Crystallized intelligence is exactly what \nit sounds like: your collection of skills, facts, and wisdom, and your ability to use them when you need to.   \n \nThose who participated in the GJP were not a valid sample of the population\u2014these are people who raised \ntheir hand to make lots of forecasts in return for a $250 gift certificate from Amazon.com. The regular \nforecasters scored higher than about 70 percent of the population on intelligence tests. That translates \nroughly into an average intelligence quotient (IQ) of 108-110 where the average of the population is 100. The \nsuperforecasters scored higher than about 80 percent of the population, or an average IQ range of 112-114. \nThere is a much bigger gap between the overall population and regular forecasters than there is between \nthose forecasters and the superforecasters.   \n   \nKeith Stanovich, professor emeritus of applied psychology and human development at the University of \nToronto, distinguishes between IQ and what he calls \u201cRQ,\u201d or rationality quotient.9 The correlation coefficient \nbetween the two is a relatively low .20 to .35. Those with high RQ\u2019s exhibit adaptive behavioral acts, efficient \nbehavioral regulation, sensible goal prioritization, reflectivity, and the proper treatment of evidence. These \nqualities are very consistent with those of the superforecasters.  \n \nJonathan Baron, a professor of psychology at the University of Pennsylvania and a colleague of Tetlock\u2019s, \ncoined the term \u201cactive open-mindedness.\u201d10 Those who are actively open-minded seek views that are \ndifferent than their own and consider them carefully. Tetlock and Gardner suggest that if they had to reduce \nsuperforecasting to a bumper sticker, it would read, \u201cBeliefs are hypotheses to be tested, not treasures to be \nguarded.\u201d   \n \n"
    },
    {
      "number": 6,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n6 \nThe Big Five is one of the most widely-accepted personality tests. Subjects are tested for five personality \ntraits: openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism. The tests \nrevealed that superforecasters score high in openness to experience, which suggests a preference for \ncognitive variety and intellectual curiosity. Superforecasters are interested in the world and are willing explorers. \n \nSuperforecasters also spend time thinking about their own process and constantly seek to improve. When \ncollaborating, the superforecasters often leave lots of comments in their online discussions, which allow them \nto recreate their thought processes and improve them when possible. Timely and accurate feedback is an \nessential element of improvement. Superforecasters embrace feedback.     \n \nThe superforecasters rarely use sophisticated mathematical models to make their forecasts, but they are \nuniformly highly numerate. Comfort with numbers is a prerequisite for making good forecasts but fancy \nquantitative models are not.  \n \nMethods of Forecasting. Superforecasters, similar to the foxes in Tetlock\u2019s study of expert political \njudgment, tend to be pragmatic in their methods. Rather than looking at all aspects of the world through a \nsingle lens, good forecasting requires considering multiple points of view. Charles Munger, vice chairman of \nBerkshire Hathaway, captures this concept well with the mental models approach. Says Munger, \u201cWell, the \nfirst rule is that you\u2019ve got to have multiple models\u2014because if you just have one or two that you\u2019re using, the \nnature of human psychology is such that you'll torture reality so that it fits your models, or at least you\u2019ll think \nit does.\u201d11 \n \nKahneman has popularized the notion of two systems of the mind. System 1 is fast, automatic, and difficult to \ntrain. System 2 is slow, deliberate, and purposeful. One of the important conclusions from this research is that \nwe commonly rely on our fast system when we should recruit our slow system.12 Tetlock and Gardner call the \nfast system the \u201ctip-of-your-nose perspective,\u201d because it is unique to each of us. Superforecasters have a \nfirm sense of when they need to engage System 2.   \n \nThere is good evidence that the aggregation of diverse points of view, done correctly, improves the accuracy \nof forecasts. James Surowiecki provides ample illustrations of this idea in his bestselling book, The Wisdom of \nCrowds.13 You can gain from diversity by capturing the views of different individuals, for example, investors in a \nstock market, or by aggregating multiple views in your head.  \n \nTetlock and Gardner use the metaphor of a dragonfly\u2019s eye. Each eye has up to 30,000 individual lenses \naimed in slightly different directions that provide the dragonfly\u2019s brain with massive input. The result is \nextraordinary visual acuity, allowing the dragonfly to nab small, fast-moving insects.  \n \nThe authors find that superforecasters, similar to the dragonfly, are able to consider and synthesize multiple \npoints of view. They also emphasize that \u201caggregation doesn\u2019t come to us naturally.\u201d We are generally content \nwith our own beliefs and see no reason to entertain alternative thoughts. Sometimes the answer is to survey \nthe views of others and ask them to criticize your view. Other times you can simply think about the same topic \nat different times and create a crowd within your head. No matter how you get there, taking in the views of \nothers is valuable. \n \nGood forecasters think in probabilities, but there\u2019s nothing easy about that. Tetlock and Gardner propose that \nwe come out of the factory with three settings on our dial of probability: it\u2019s going to happen; it\u2019s not going to \nhappen; and maybe. They suggest this worked fine for our ancestors. \u201cIs that a lion? YES = run! MAYBE = \nstay alert! NO = relax.\u201d  \n"
    },
    {
      "number": 7,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n7 \nForecasters commonly use 50 percent to represent \u201cmaybe,\u201d and sure enough the forecasters who used 50 \npercent the most frequently were less accurate than the average. \n \nSuperforecasters provide more finely detailed forecasts than the other forecasters. Instead of 60, 70, 80 \npercent they are more likely to use 70, 75, 80 percent, or even 70, 71, 72 percent. This precision was not for \nshow: the more granular forecasts were more accurate than the less granular ones.       \n \nPhilosophers suggest a distinction between cases where you don\u2019t know the outcome but the possibilities are \nknowable\u2014the roll of a die for instance\u2014and cases where you don\u2019t know and the alternatives are \nunknowable.14 Superforecasters recognize that the cloudier the outlook for the answer, the more it benefits \nthem to stay in the zone of \u201cmaybe.\u201d  \n \nWe will come back to probability when we discuss tools, but it is again worth underscoring the value of \nfeedback. One of the surest ways to improve calibration, the alignment of subjective and objective probabilities, \nis through timely and accurate feedback. Part of the reason that weather forecasters are better than financial \nforecasters is that they quickly see whether their predictions are accurate.15 \n \nWe live in a dynamic world, so new information that should change our assessments of probabilities arrives all \nthe time. Superforecasters have more accurate initial predictions on average than the regular forecasters do, \nbut they also update their views more often. Such updating requires an open mind, but also comes with the \nrisk of under- or overreacting. \n \nTetlock and Gardner offer three reasons that forecasters underreact to new information. To start, sometimes \nwe are so busy that novel information merely slips our attention. We also may take our eye off of the original \nquestion and dwell on a simpler or slightly different one. So the new information may not appear relevant to \nthe question in our minds even though it is relevant to the question at hand. Finally, and probably most likely, \nis belief perseverance. This is typically accompanied by confirmation bias\u2014actively seeking information that \nsupports our view and dismissing views counter to it.16      \n \nBut it\u2019s also possible to overreact to new information. One reason is that we take into account irrelevant \ninformation. People may base their initial estimate on solid reasoning, but subsequently place weight on \nadditional information that has no bearing on the issue at hand. A second reason for overreaction is a lack of \ncommitment. Say you buy a mutual fund after having done research convincing yourself that the portfolio \nmanager is skillful and that her strategy will do well over time. If you sell at the first bump in performance, you \nare showing a lack of commitment. \n \nUse of Bayes\u2019s Theorem is a formal way to update probabilities, but it turns out that the superforecasters \ngenerally don\u2019t use it. This is true even for those steeped in the math of the theorem. There is no easy way to \ncorrectly update views, but we know that superforecasters spend a lot of time thinking about how to do it well.  \n \nWe think intuitively most of the time. Our intuitive system tends to use rules of thumb, or heuristics. For \nexample, the availability heuristic suggests that we deem events that are easy to remember, because of \nvividness or recency, to be more numerous than events of equal frequency that are not as simple to recall. \nOther heuristics include representativeness and anchoring.17  \n \nHeuristics are wonderful because they save us a great deal of time. But they also come with biases that can \nundermine the quality of our judgments. For example, people are unjustifiably more fearful of flying after \nhaving heard of an aviation accident.   \n \n"
    },
    {
      "number": 8,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n8 \nSuperforecasters have an above-average awareness of these biases and try to manage them. Both the \nthinking styles and forecasting methods of superforecasters help address bias. The melding of humans and \ncomputers is another path to improved decisions. Done correctly, the output of man plus machine can exceed \nman or machine in certain domains.  \n \nDavid Ferrucci is an artificial intelligence expert who was in charge of creating Watson, the computer that beat \nchampions Ken Jennings and Brad Rutter in the game show Jeopardy! He sees a role for human judgment \neven with the rise of the robots. But he also believes that computers can help overcome human bias. \u201cSo what \nI want is that human expert paired with a computer,\u201d said Ferrucci, \u201cto overcome the human cognitive \nlimitations and biases.\u201d    \n \nWork Ethic. Carol Dweck, a professor of psychology at Stanford University, is best known for her work on \nmindset, or a way of thinking. She suggests individuals can be placed on a continuum, with a \u201cfixed mindset\u201d \nat one extreme and a \u201cgrowth mindset\u201d at the other, based on their implicit belief about the source of ability. \nPeople with a fixed mindset believe that ability is innate and therefore can\u2019t be budged. People who say, \u201cI\u2019m \njust bad at math,\u201d have a fixed mindset. Those with a growth mindset believe that ability is the result of hard \nwork and effort and therefore can improve over time.    \n \nSuperforecasters fall on the growth mindset side of the continuum. They believe that there is always room for \nimprovement and seek ways to get better. In discussing the results of one of her experiments, Dweck noted, \n\u201cOnly people with a growth mindset paid close attention to information that could stretch their knowledge. Only \nfor them was learning a priority.\u201d18   \n \nAnother quality that the superforecasters have is grit, a term popularized by Angela Duckworth, another one of \nTetlock\u2019s colleagues at the University of Pennsylvania.19 Grit is perseverance in the service of long-term goals. \nIt entails the ability to overcome failure and obstacles along the way of achieving an objective.  \n \nCombine a growth mindset and grit and you have an outstanding formula for personal development and \nimprovement. Tetlock and Gardner call the combination \u201cperpetual beta.\u201d A product in beta is nearly complete \nbut has room for improvement. Perpetual beta suggests a desire for ongoing improvement. Exhibit 1 \nsummarizes the composite portrait of a superforecaster. \n \n \n \n"
    },
    {
      "number": 9,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n9 \nExhibit 1: Composite Portrait of the Modal Superforecaster \uf402\n Philosophic Outlook \n\u2212 Cautious: Nothing is certain \n\u2212 Humble: Reality is infinitely complex \n\u2212 Nondeterministic: What happens is not meant to be and does not have to happen \uf402\n Abilities and Thinking Styles \n\u2212 Actively open-minded: Beliefs are hypotheses to be tested, not treasures to be protected \n\u2212 Intelligent and knowledgeable, with a \u201cneed for cognition\u201d: Intellectually curious, enjoy puzzles \nand mental challenges \n\u2212 Reflective: Introspective and self-critical \n\u2212 Numerate: Comfortable with numbers \uf402\n Methods of Forecasting \n\u2212 Pragmatic: Not wedded to any idea or agenda \n\u2212 Analytical: Capable of stepping back from the tip-of-your-nose perspective and considering other \nviews \n\u2212 Dragonfly-eyed: Value diverse views and synthesize them into your own \n\u2212 Probabilistic: Judge using many grades of maybe \n\u2212 Thoughtful updaters: When facts change, they change their minds \n\u2212 Good intuitive psychologists: Aware of the value of checking thinking for cognitive and emotional \nbiases \uf402\n Work Ethic \n\u2212 A growth mindset: Believe it\u2019s possible to get better \n\u2212 Grit: Determined to keep at it however long it takes \nSource: Philip E. Tetlock and Dan Gardner, Superforecasting: The Art and Science of Prediction (New York: Crown Publishers, 2015), 191-192. Used \nby permission. \n \nThese elements of a superforecaster may be valuable for adding structure to hiring processes and \nperformance evaluation. Further, leaders of organizations in the forecasting business should give careful \nconsideration to creating an environment conducive to good judgment.     \n \nManage Interaction \n \nThe objective of the IARPA tournament was to make accurate forecasts, and the GJP had already proven that \nit could do that. The next question was whether working in teams would improve accuracy. To find out, in year \none the GJP researchers randomly assigned some forecasters to work in teams and provided them with tips \non how to work together effectively. Others were to work alone. This was before the scientists had identified \nthe superforecasters. \n \nThe results were clear: teams were on average 23 percent more accurate than individuals. Exhibit 2 shows \nthe statistical results for the first two years. Recall that a lower Brier score is better than a higher one. \n \n \n \n"
    },
    {
      "number": 10,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n10 \nExhibit 2: Forecasting Teams Outperformed Individuals  \n \nSource: Based on Barbara Mellers, Lyle Ungar, Jonathan Baron, Jaime Ramos, Burcu Gurcay, Katrina Fincher, Sydney E. Scott, Don Moore, Pavel \nAtanasov, Samuel A. Swift, Terry Murray, Eric Stone, and Philip E. Tetlock, \u201cPsychological Strategies for Winning a Geopolitical Forecasting \nTournament,\u201d Psychological Science, Vol. 25, No. 5, May 2014, 1106-1115. \nNote: Error bars represent plus and minus two standard errors. \n \nMost organizations use teams to get their work done. In some sectors, including investment management, the \ntrend shows an increasing use of teams.20 But the research also reveals that there are pros and cons to \nworking in teams. The pros are that individuals can share information, and aggregation tends to lead to more \naccurate forecasts. The cons are that team members may be tempted to loaf, and the team may fall into \ngroupthink, failing to capture the value of cognitive diversity.  \n \nIn year two, the researchers created teams of superforecasters. Each team had a dozen members, but about \nfive or six individuals did most of the work. Once again, the teams were trained on how to work together, and \nbecause the members did not meet face-to-face the project coordinators created forums for them to \ncommunicate.  \n \nThe outcome was remarkable. Those who were good enough to achieve superforecaster status in year 1 were \n50 percent more accurate, on average, in year 2 as part of a team. Year three was more of the same. (See \nExhibit 3.) Indeed, these superteams were 15 to 30 percent better than prediction markets, a high standard to \nexceed.  \n \n \n \n \n \n \n \n \n \n \n \n0.10\n0.05\n0.00\n-0.05\n-0.10\n-0.15\nYear 1\nYear 2\nIndividual Team\nIndividual Team\nMean Standardized Brier Score\n0.15\n"
    },
    {
      "number": 11,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n11 \nExhibit 3: Superforecasters Are Even Better in Teams  \n \nSource: Based on Barbara Mellers, Eric Stone, Terry Murray, Angela Minster, Nick Rohrbaugh, Michael Bishop, Eva Chen, Joshua Baker, Yuan Hou, \nMichael Horowitz, Lyle Ungar, and Philip Tetlock, \u201cIdentifying and Cultivating Superforecasters as a Method of Improving Probabilistic Predictions,\u201d \nPerspectives on Psychological Science, Vol. 10, No. 3, May 2015, 267\u2013281.  \nNote: Error bars represent plus and minus one standard error. \n \nThis is not to say that all went smoothly. Initially, the superforecasters were cautious, restraining comments \nthat would be perceived to be too critical of the assessments of others. And not all of the superforecasters \nhad great social skills. In most cases, however, the teammates figured out ways to engage one another in \nconstructive confrontation that allowed the groups to improve performance. The superforecasters were invited \nto get together in person after years two and three, and many of them reported that the added human \ndimension and sense of commitment were helpful. \n \nNotwithstanding the success of the superteams, the GJP researchers have reservations about how well the \nformula would apply in a corporate setting. Identifying individuals as \u201csuper,\u201d getting employees from different \nparts of the firm to work together, and interpersonal dynamics might all create friction.  \n \nThe main lesson is that interaction among a diverse group, especially those with the profile of a \nsuperforecaster, can be very effective if managed properly. Companies that seek diversity should also be \nprepared to train employees in how to manage diversity.21  \n     \nTrain Effectively \n \nOne remarkable finding from the GJP is that relatively little training, about an hour, can improve results by 10 \npercent. (See Exhibit 4.) For individuals, the training focused on sharpening probabilistic reasoning and \nremoving cognitive biases. One example is how we tend to use the \u201cinside\u201d versus the \u201coutside\u201d view. With the \ninside view, we tend to rely on our own information and perception. The outside view considers a problem as \nan instance of a larger reference class and appeals to the base rate of past occurrence. While both are \nimportant in good judgment, psychologists have shown that we commonly rely too much on the inside view. \nMaking individuals aware of the outside view can help reduce this bias. \n \n \n \nMean Standardized Brier Score\n0.1\n0.0\n-0.1\n-0.2\n-0.3\n-0.4\nYear 1\nYear 2\nYear 3\nSupers\nTop-Team \nIndividuals\nAll Others\n"
    },
    {
      "number": 12,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n12 \nExhibit 4: Forecasters Benefit from Debiasing Training  \n \nSource: Based on Barbara Mellers, Lyle Ungar, Jonathan Baron, Jaime Ramos, Burcu Gurcay, Katrina Fincher, Sydney E. Scott, Don Moore, Pavel \nAtanasov, Samuel A. Swift, Terry Murray, Eric Stone, and Philip E. Tetlock, \u201cPsychological Strategies for Winning a Geopolitical Forecasting \nTournament,\u201d Psychological Science, Vol. 25, No. 5, May 2014, 1106-1115. \nNote: Error bars represent plus and minus two standard errors. \n \nFor groups, the training addressed how to work together. The goal is to strike a balance between conflict and \nharmony. Too much conflict and the group dynamics break down. No one wants to interact. Too much \nharmony leads to a false consensus, or even groupthink. The right balance is an atmosphere of constructive \ncriticism.       \n \nOverweight Elite Forecasters or Extremize Estimates  \n \nThe simplest way to capture the wisdom of crowds is to aggregate the estimates of a large number of people \nwith diverse views. For example, you can let lots of people examine a jar filled with jelly beans and ask each of \nthem to provide an estimate of the total. The collective prediction will beat the average individual predictions \nand will generally be very close to the actual number of jelly beans.  \n \nThe statisticians working on the GJP figured out two ways to improve the quality of forecasts.22 The first is to \nplace more weight on what the superforecasters say. The intuition behind this is straightforward; give those \nwho predict more accurately a greater voice.  \n \nThe second is the application of an algorithm to \u201cextremize\u201d answers. Imagine five individuals with different \npoints of view who all estimate the probability of an outcome to be 75 percent. The extremizing algorithm \nwould push that probability closer to 100 percent. Likewise, a group estimate with a low probability would be \nadjusted closer to zero percent. How far the algorithm pushes the aggregate forecast toward 0 or 100 is a \nfunction of the diversity and sophistication of the pool of forecasters.  \n \nThe idea is to capture unshared information.23 Here\u2019s one way to think about it. Each of our diverse \nforecasters who come up with a probability of 75 percent uses some information that\u2019s common to all of them \nand some that\u2019s unique to them. What would happen if each of them knew all of the information? It would \nstrengthen their confidence, moving their collective estimate toward 100 percent. So the extremizing algorithm \ncaptures what would happen if diverse individuals could share all of their information with the group.        \n0.10\n0.05\n0.00\n-0.05\n-0.10\n-0.15\nYear 1\nYear 2\nNone\nTraining\nNone\nTraining\nMean Standardized Brier Score\n"
    },
    {
      "number": 13,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n13 \nA Superforecaster\u2019s Tools    \n \nWe have covered what makes for a superforecaster and the elements of an accurate prediction. But we have \nglossed over some of the essential tools necessary to come up with good forecasts. We now cover those \ntools, including defining a good question, keeping score, and approaches to forecasts. \n \nGood Questions. The first step in coming to useful answers is to ask good questions. A good question has a \nclear outcome within a specified period of time and addresses an issue that is relevant to the world. Let\u2019s look \nat each of these components. \n \nBecause keeping score is essential to developing the skill of foresight, questions must have clear answers. \nConsider the simple example of weather. Weather forecasters predict temperature, precipitation, and winds. \nThey can measure each of these and compare the prediction to the outcome. Likewise, questions in the GJP \nand those from the intelligence community should have clear answers.  \n \nQuestions must also have a set time frame. One key difference between Tetlock\u2019s prior research on expert \njudgment, which showed poor results, and the latest results from the forecasting tournament, which paint a \nmore optimistic picture, is the time frame. In the prior research, the questions went out three to five years \nwhereas in the IARPA tournament they average about a few months to a year. Predicting outcomes in the \npolitical, economic, and social realms over three to five years is very hard for anyone. Predictions in shorter \ntime frames are more feasible and yet still relevant for business and policy decisions.  \n \nOpen-ended predictions are of limited value. Tetlock and Gardner discuss a letter sent in November 2010 to \nBen Bernanke, then chairman of the Federal Reserve. Signed by nearly two dozen economists, investors, and \ncommentators, the letter suggested that quantitative easing should be \u201creconsidered and discontinued\u201d and \nnoted the risk of \u201ccurrency debasement and inflation.\u201d In the fall of 2014, reporters at Bloomberg contacted \nsome of the signees and found that \u201call of those who commented stood by the letter\u2019s contents.\u201d With no time \nframe or objective way to score the prediction, the letter provides no way to keep score. \n \nFinally, good questions are also relevant. Some questions may be a subset of a group of questions that allow \nyou to assess a larger, but more difficult, issue. Tetlock and Gardner suggest a useful question should pass \nthe smack -the-forehead test: \u201cwhen you read the question after time has passed, you smack your forehead \nand say, \u2018If only I had thought of that before!\u2019\u201d  \n \nHere\u2019s an example of a question from the forecasting tournament. Note that it has a clear outcome, specified \ntime period, and relevance:24  \n \n\u201cWill Italy\u2019s Silvio Berlusconi resign, lose reelection/confidence vote, or otherwise vacate office before 1 \nJanuary 2012?\u201d \n \nTetlock and Gardner make an additional point on questions. They suggest that the skills in asking good \nquestions may be different than the skills in answering them. The superquestioners might not be the same as \nthe superforecasters. Thoughtful organizations should seek to cultivate skills in both capacities. \n \nKeeping Score. We have already mentioned the Brier score, the main method that psychologists use to \nmeasure results. If you have a prediction with a specific probability and an outcome, you are in a position to \ngrade the predictions. Since Brier scores reflect the distance from a forecast to the outcome, lower figures are \nbetter. Superforecasters were those in the tournament who earned the lowest Brier scores.  \n \n"
    },
    {
      "number": 14,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n14 \nKeeping score is crucial because it provides feedback and therefore an opportunity to learn. Forecasters want \nto improve in two ways. One way is called \u201ccalibration,\u201d which means that your forecasts line up with the \noutcomes. For example, you are well calibrated if you say that certain events will occur with a 40 percent \nprobability and they actually happen 40 percent of the time.  \n \nAnother way to improve is what the authors call \u201cresolution.\u201d Resolution means that when you are sure \nsomething is not going to happen, it doesn\u2019t happen, or when you\u2019re sure it will happen, it does. It\u2019s a \nmeasure of conviction. Good calibration and resolution are correlated, but they are distinct. You want to \nsharpen your skills in both ways. \n \nTetlock is fond of the phrase \u201cvague verbiage slows learning cycles.\u201d26 In our day-to-day conversations we use \nlots of phrases\u2014maybe, possible, probable, might happen\u2014that are ambiguous. In one famous illustration, in \n1951 the U.S. intelligence community produced a report saying that the Soviet Union\u2019s invasion of Yugoslavia \nwas a \u201cserious possibility.\u201d Sherman Kent, a professor of history at Yale who was then serving at the Central \nIntelligence Agency, asked some members of his team what \u201cserious possibility\u201d meant. Even though they all \nagreed to use the term in their report, one analyst said it meant an 80 percent probability and another said 20 \npercent.  \n \nThe lesson from this story is to use specific probabilities and time horizons whenever possible and to keep \ntrack of those forecasts. Numerical probabilities dismiss the risk of misinterpretation or misunderstanding that \nKent faced and provide forecasters with the feedback they need to improve.      \n \nApproaches to Forecasts. In studying how superforecasters approach their task, Tetlock and his colleagues \nnoticed a few methods that may be useful beyond the tidy confines of the forecasting tournament. \n \nThe first is the idea of question triage. In times of medical emergency such as war, doctors and nurses sort \ncasualties based on their injuries. This process is called triage. Patients who require immediate care receive \npriority over those who are expected to live and those who have no chance of survival. Similarly, you can sort \nquestions. Some are too easy and others too hard. The priority should go to questions that are between those \nextremes where \u201ceffort pays off the most.\u201d  \n \nSuperforecasters also use specific techniques to answer some bigger, but answerable, questions. One is \nassociated with Enrico Fermi, a professor at the University of Chicago who worked on the Manhattan Project \nand won the Nobel Prize in physics. It is effectively a back-of-the-envelope calculation. Fermi used the \napproach to estimate the strength of an atomic bomb test, but you can use the technique for any question for \nwhich there is an answer. \n \nIn Superforecasting, Tetlock and Gardner use a popular example of a Fermi problem: How many piano tuners \nare there in Chicago? The tactic is to break the big question into a series of smaller, and easier, questions. \nYou might estimate the population of Chicago, assess the number of households, judge how many pianos \nthere are per household, consider the number of schools and places of worship, determine how often pianos \nneed to be tuned, and contemplate how long it takes to tune a piano and how many hours a piano tuner might \nwork. It turns out there are about 250-300 piano tuners in Chicago.27  \n    \nAnother technique is called question clustering. The idea is that the big question we want to answer\u2014\u201cWill \nNorth Korea initiate a war?\u201d\u2014is too big, and the small questions\u2014\u201cWill North Korea launch a multistage \nrocket by some date?\u201d\u2014are too small. Tetlock and Gardner suggest that a cluster of small questions, each of \nwhich you can update, provide insight into answering the bigger question. They use the metaphor of the \n"
    },
    {
      "number": 15,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n15 \npainting technique, pointillism, which consists of adding dots to the canvas. No dot by itself means much, but \ntogether they paint an evocative picture. \n \nTo gauge the likelihood of North Korean aggression, they propose that you might ask questions about missile \nlaunches, nuclear tests, cyber-attacks, and artillery shelling. The patterns of probabilities provide a path to \ntackling some of the big questions. Here again, it\u2019s good to remember that those who come up with the \nquestions and those who answer need not be the same. \n \nLeadership \n \nTetlock and Gardner suggest that if you ask people to list the qualities of a strong leader, you will hear \nadjectives such as confident, decisive, and visionary. But as we have seen, those descriptions don\u2019t fit the \nsuperforecasters very well. Leaders are supposed to be in perpetual action mode, while superforecasters \nseem to be in perpetual learning mode.  \n \nThe authors refer to the wisdom of Helmuth von Moltke, the chief of staff of the Prussian Army for three \ndecades and considered by many to be the inventor of the modern method of directing troops in the field. \u201cIn \nwar, everything is uncertain,\u201d said Moltke, recognizing the limits of knowledge. And acknowledging the \nnecessity of flexibility, he observed, \u201cNo plan of operations extends with certainty beyond the first encounter \nwith the enemy\u2019s main strength.\u201d  \n \nMoltke\u2019s point, which was eventually embraced by the German army, is that leaders must think and change \ntheir course of action as necessary. Orders could be questioned, even criticized, if there was a better way. The \nmessage was to be actively open-minded. A German military manual notes, \u201cThe art of leadership consists of \nthe timely recognition of circumstances and of the moment when a new decision is required.\u201d  \n \nDavid Petraeus, a retired four-star U.S. Army general, embraced many of Moltke\u2019s themes in a contemporary \ncontext. He supported sending his officers to top universities to pursue graduate studies in order to hear \npoints of view that were different from their own. This creates vital mental flexibility and sounds a lot like what \nsuperforecasters do.    \n \nBut Petraeus also recognized that leading requires thinking and doing. He said that a leader \u201cneeds to figure \nout what\u2019s the right move and then execute it boldly.\u201d Humility should make a leader think carefully about what \nhe or she is doing, and confidence should give an individual the strength to act.  \n \nUltimately, we want our leaders to be confident, decisive, and visionary. But the lesson of Superforecasting is \nthat thinking is an important precursor to action. Further, circumstances change and it is vital to update views \nand change strategic course appropriately.    \n  \n \n \n \n \n \n \n \n \n \n"
    },
    {
      "number": 16,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n16 \nDoubts \n \nPhil Tetlock is highly respected in his field and, like the superforecasters he studies, is in constant search of \nimprovement. Two of his friends and colleagues, Daniel Kahneman and Nassim Taleb, offer some challenges \nto the skill Tetlock has identified in forecasting. \n \nKahneman\u2019s challenge relates to \u201cscope insensitivity.\u201d The idea is that the questions we face often evoke a \nfeeling of emotion. Scientists can measure that affect with money.  \n \nFor example, researchers might ask you how much money you would be willing to pay to save 2,000 \nmigratory birds from dying from hazards, including oil spills, wetlands destruction, or residue from herbicides \nand pesticides. Subjects say about $80. You feel bad about the situation and are willing to pay some amount \nof money to help.  \n \nHere\u2019s the issue: When researchers asked subjects how much they would be willing to pay to save 20,000 \nbirds, the answer was $78. And for 200,000 birds the response was $88.28 The dollar sum reflects how \npeople feel about the scenario, not how much they would be willing to pay to save each bird. There is no cost-\nbenefit analysis. Subjects think using System 1 and as a result are insensitive to the scope.  \n \nKahneman thought that a similar type of scope insensitivity might affect the GJP forecasters. Specifically, he \nthought scope insensitivity would have to do with the time frame. For example, if you are asked how likely it is \nthat a particular dictatorial regime will fall you will focus on the probability and fail to properly consider the time \nframe.    \n \nRunning the numbers, the GJP scientists found that indeed many of the forecasters were scope insensitive. \nWhen asked the probability that the regime of Syrian President Bashar al-Assad would fall, regular forecasters \nassigned a 40 percent probability that it would happen within 3 months and a 41 percent chance within 6 \nmonths. They assigned essentially the same probability to a period twice as long as the first. \n \nBut the superforecasters did much better. Their probabilities were 15 percent for 3 months and 24 percent for \n6 months. That\u2019s not perfect sensitivity, but it is a lot closer than the regular forecasters. Tetlock and Gardner \ninterpret this result as the ability of superforecasters to engage System 2 thinking more readily than the \nmajority of forecasters. Indeed, they argue that superforecasters have internalized this way of thinking to the \npoint that it has become automatic. \n \nNassim Taleb has popularized the concept of a black swan event, which comes as a surprise, is consequential, \nand is explained after the fact. Tetlock and Gardner say that \u201cTaleb insists that black swans, and black swans \nalone, determine the course of history.\u201d If so, IARPA\u2019s forecasting tournament is of little value. \n \nTetlock and Gardner offer some thoughts to counter this view. The first has to do with the definition of a black \nswan. If we know what a distribution of outcomes looks like, even those with extreme events, then we are \ndealing with \u201cgray swans,\u201d not black swans. Scientists have done a lot of work classifying the distributions of \ncertain phenomena, including earthquakes, terrorist acts, and power-grid failures. While predicting a specific \noutcome is very difficult, scientists have a general understanding of how these systems behave. Taleb calls \nthese \u201cmodelable extreme events\u201d and they capture much of what is of interest to us.29  \n \nBlack swans and gray swans are highly improbable outcomes. As a result, there are simply not enough data \nfrom the forecasting tournament to conclude that superforecasters are either good or bad at spotting them. In \n"
    },
    {
      "number": 17,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n17 \nfact, even if the tournament ran for decades it would be hard to know. So those seeking accurate forecasts of \nblack or gray swan events will have to look elsewhere.       \n \nBut the fact is that the forecasting tournament does address a lot of issues that are relevant for executives, \ninvestors, and policymakers. While black swans have a large impact on the world, so do the accumulation of \nlots of small events. Thinking about the future is not black swan or nothing; there is plenty of room to consider \noutcomes that have smaller impact. \n \nOne point on which Tetlock, Kahneman, and Taleb agree is that forecasting many years into the future is very \ndifficult. There are severe limits on predictability beyond a certain period of time. The essential issue is \nwhether there is value in sharp forecasts for horizons less than a year. Most professionals would answer with a \nresounding yes. \n \nSummary \n \nHumans have a deep-seated desire to anticipate the future. The demand for forecasts is met by a supply of \nseers, pundits, and experts expounding on what will happen next. But we generally don\u2019t measure the quality \nof predictions, and when it has been done the results are unimpressive. \n \nSuperforecasting shows that prediction may not be so futile after all. The Good Judgment Project, part of a \nforecasting tournament sponsored by the U.S. intelligence community, revealed that some forecasters are not \nonly good but consistently good. Using the best ideas from psychology and careful measurement, the GJP \nteam has been able to provide essential lessons for anyone in the prediction business. Here are some of the \nmain conclusions: \n \uf402\n Forecasting skill exists. The researchers found that a small percentage of the forecasting population \nwere much more accurate than average and consistently so. They were able to find the superforecasters \nbecause they welcomed a large sample of forecasters, asked questions with time frames of a year or \nless, and kept track of the responses. \n \uf402\n Way of thinking is vital. Closer analysis of the superforecasters shows that they are bright, but not \nextraordinarily so. What distinguishes them from regular forecasters is the way they think. \nSuperforecasters are actively open-minded, nondeterministic, intellectually humble, numerate, thoughtful \nupdaters, and hard working.  \n \uf402\n Teams. When superforecasters interact with one another, their predictions improve. But there are pros \nand cons to working in teams. The pros include more information and the ability to harness the power of \naggregation. The cons are social loafing and the risk of groupthink. Teams that do the best have \nmembers who have been given instruction and training on how to work together.  \n \uf402\n Training. Training in methods to de-bias forecasts and to collaborate effectively improves outcomes. \nProper training has content that is valuable and processes that can be implemented immediately. Training \nwithout implementation is a waste. \n \uf402\n Leadership. At first blush, the qualities associated with leadership appear antithetical to those of the \nsuperforecasters. The way to reconcile the two is to acknowledge that proper, even bold, action requires \ngood thinking. And the best leaders recognize that even the best laid plans need to be constantly revised \nbased on the conditions.      \n \n"
    },
    {
      "number": 18,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n18 \nAppendix: Keeping Score with Brier  \n \nPsychologists commonly use the Brier score as a method for gauging the accuracy of probabilistic forecasts. \nGlenn Brier, a meteorologist, developed the score in the 1950s.30 In its simplest form, the Brier score \nmeasures the square of the forecast error, or (forecast \u2212 outcome)2. For binary events, the value of the \noutcome is 1 if the event occurs and 0 if it does not. As in golf, a lower score is better. \n \nYou can express a Brier score either on a scale of 0 to 1, or 0 to 2, depending on the calculation. We follow \nBrier\u2019s original approach and place our results on a scale of 0 to 2. When calculating the Brier score this way, \nyou consider the squared forecast error for both the event and the non-event. \n \nExhibit 5 shows a meteorologist\u2019s probabilistic forecasts for whether it will rain over the next four days. For \nexample, on Day 2, she forecasts an 80 percent probability that it will rain. Likewise, we can say she \nforecasts a 20 percent probability that it will not rain. Because it did rain, we place a 1 in the outcome column \nbelow \u201cRain\u201d and a 0 in the \u201cNo Rain\u201d column. Her Brier score for that day was 0.08. For multiple forecasts, \nthe overall Brier score is the mean of the scores for each forecast. The meteorologist\u2019s overall Brier score \ncomes to 0.25. \n \nExhibit 5: Calculation of Brier Score \n \nSource: Credit Suisse. \n \nThe scale from 0 to 2 has a nice feature. Random guesses have a Brier score of exactly 0.50. Exhibit 6 \nshows the Brier scores for an event that occurs (\u201cRain\u201d) for subjective probabilities from 0 to 100 percent. \nSuperforecasters have Brier scores of around 0.20 \u2013 0.25 and in some exceptional cases can achieve scores \nin the teens. \n \nExhibit 6: Brier Scores of Event That Occurs for Various Subjective Probabilities \n \nSource: Credit Suisse. \nDay\nForecast Outcome Forecast Outcome\nCalculation\nResult\n1\n30%\n0\n70%\n1\n= (0.3-0)2 +(0.7-1)2\n0.18\n2\n80%\n1\n20%\n0\n= (0.8-1)2 +(0.2-0)2\n0.08\n3\n60%\n0\n40%\n1\n= (0.6-0)2 +(0.4-1)2\n0.72\n4\n100%\n1\n0%\n0\n= (1.0-1)2 +(0.0-0)2\n0.00\nMean\n0.25\nRain\nNo Rain\nBrier Score\n0.0\n0.5\n1.0\n1.5\n2.0\n1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0\nBrier Score\nForecast\n"
    },
    {
      "number": 19,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n19 \nEndnotes \n \n1 Philip E. Tetlock and Dan Gardner, Superforecasting: The Art and Science of Prediction (New York: Crown \nPublishers, 2015), 191. \n2 Philip E. Tetlock, Expert Political Judgment: How Good Is It? How Can We Know? (Princeton, NJ: Princeton \nUniversity Press, 2005). \n3 David Ignatius, \u201cMore Chatter Than Needed,\u201d Washington Post, November 1, 2013.  \n4 Daniel Kahneman and Gary Klein, \u201cConditions for Intuitive Expertise: A Failure to Disagree,\u201d American \nPsychologist, Vol. 64, No. 6, September 2009, 515-526. \n5 Daniel Kahneman, Thinking, Fast and Slow (New York: Farrar, Straus and Giroux, 2011). \n6 Philip E. Tetlock, \u201cThe Good Judgment Project: Can We Improve Probability Judgments of Possible Futures?\u201d \nPresentation at the Credit Suisse Thought Leader Forum, June 11, 2014.  \n7 To learn more of this mistake, see Phil Rosenzweig, The Halo Effect . . . and the Eight Other Business \nDelusions That Deceive Managers (New York: Free Press, 2007).  \n8 Kahneman, 212. \n9 Keith E. Stanovich, What Intelligence Tests Miss: The Psychology of Rational Thought (New Haven, CT: \nYale University Press, 2009). See also Michael J. Mauboussin and Dan Callahan, \u201cIQ versus RQ: \nDifferentiating Smarts from Decision-Making Skills,\u201d Credit Suisse Global Financial Strategies, May 12, 2015.  \n10 Jonathan Baron, \u201cBeliefs about Thinking,\u201d in James F. Voss, David N. Perkins, and Judith W. Segal, eds., \nInformal Reasoning And Education (Hillsdale, NJ: Lawrence Erlbaum Associates, Inc., 1991), 169-186. \n11 Charles Munger, \u201cA Lesson on Elementary Worldly Wisdom as It Relates to Investment Management & \nBusiness,\u201d Outstanding Investor Digest, Vol. 10, No. 1 & 2, May 5, 1995, 49-63.  \n12 Michael J. Mauboussin, Think Twice: Harnessing the Power of Counterintuition (Boston, MA: Harvard \nBusiness Press, 2009). \n13 James Surowiecki, The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective \nWisdom Shapes Business, Economies, Societies, and Nations (New York: Doubleday, 2004).  \n14 This is consistent with the difference between risk and uncertainty proposed by the economist Frank Knight.  \n15 Tadeusz Tyszka and Piotr Zielonka, \u201cExpert Judgments: Financial Analysts Versus Weather Forecasters,\u201d \nJournal of Psychology and Financial Markets, Vol. 3, No. 3, 2002, 152-160. \n16 Raymond S. Nickerson, \u201cConfirmation Bias: A Ubiquitous Phenomenon in Many Guises,\u201d Review of General \nPsychology, Vol. 2, No. 2, June 1998, 175-220.  \n17 Max Bazerman, Judgment in Managerial Decision Making, 4th Ed. (New York: John Wiley & Sons, 1998).  \n18 Carol S. Dweck, Mindset: The New Psychology of Success (New York: Random House, 2006), 18. \n19 Angela Duckworth, Grit: The Power of Passion and Perseverance (New York: Scribner, 2016). \n20 Michael J. Mauboussin and Dan Callahan, \u201cBuilding an Effective Team: How to Manage a Team to Make \nGood Decisions,\u201d Credit Suisse Global Financial Strategies, January 8, 2014. \n21 Elizabeth Mannix and Margaret A. Neale, \u201cWhat Differences Make a Difference? The Promise and Reality of \nDiverse Teams in Organizations,\u201d Psychological Science in the Public Interest, Vol. 6, No. 2, October 2005, \n31-55. \n22 Jonathan Baron, Barbara A. Mellers, Philip E. Tetlock, Eric Stone, and Lyle H. Ungar, \u201cTwo Reasons to \nMake Aggregated Probability Forecasts More Extreme,\u201d Decision Analysis, Vol. 11, No. 2, June 2014, 133-\n145.  \n23 Garold Stasser and William Titus, \u201cPooling of Unshared Information in Group Decision Making: Biased \nInformation Sampling During Discussion,\u201d Journal of Personality and Social Psychology, Vol. 48, No. 6, June \n1985, 1467-1478. Also, Jennifer R. Winquist and James R. Larson, Jr., \u201cInformation Pooling: When It \nImpacts Group Decision Making,\u201d Journal of Personality and Social Psychology, Vol. 74, No. 2, February \n1998, 371-377.  \n24 Barbara Mellers, Eric Stone, Terry Murray, Angela Minster, Nick Rohrbaugh, Michael Bishop, Eva Chen, \nJoshua Baker, Yuan Hou, Michael Horowitz, Lyle Ungar, and Philip Tetlock, \u201cIdentifying and Cultivating \n"
    },
    {
      "number": 20,
      "text": " \n \nSeptember 28, 2015 \nSharpening Your Forecasting Skills \n20 \nSuperforecasters as a Method of Improving Probabilistic Predictions,\u201d Perspectives on Psychological Science, \nVol. 10, No. 3, May 2015, 267\u2013281.  \n25 Caleb Melby, Laura Marcinek, and Dani Burger, \u201cFed Critics Say \u201910 Letter Warning Inflation Still Right,\u201d \nBloomberg, October 2, 2014.  \n26 Tetlock (2014).  \n27 See https://en.wikipedia.org/wiki/Fermi_problem. \n28 William H. Desvousges, F. Reed Johnson, Richard W. Dunford, Kevin J. Boyle, Sara P. Hudson, and K. \nNicole Wilson, Measuring Nonuse Damages Using Contingent Valuation: An Experimental Evaluation of \nAccuracy, 2nd Ed. (Research Triangle Park, NC: RTI Press Publication, 2010). \n29 Nassim Nicholas Taleb, The Black Swan: The Impact of the Highly Improbable (New York: Random House, \n2007), 272. \n30 Glenn W. Brier, \u201cVerification of Forecasts Expressed in Terms of Probability,\u201d Monthly Weather Review, Vol. \n78, No. 1, January 1950, 1-3. \n \n \n \n \n"
    },
    {
      "number": 21,
      "text": " \n \nImportant information \n \nThis document was produced by and the opinions expressed are those of Credit Suisse as of the date of writing and are subject to change. It has been prepared \nsolely for information purposes and for the use of the recipient. It does not constitute an offer or an invitation by or on behalf of Credit Suisse to any person to buy \nor sell any security. Nothing in this material constitutes investment, legal, accounting or tax advice, or a representation that any investment or strategy is suitable \nor appropriate to your individual circumstances, or otherwise constitutes a personal recommendation to you. The price and value of investments mentioned and \nany income that might accrue may fluctuate and may fall or rise. Any reference to past performance is not a guide to the future.  \nThe information and analysis contained in this publication have been compiled or arrived at from sources believed to be reliable but Credit Suisse does not make \nany representation as to their accuracy or completeness and does not accept liability for any loss arising from the use hereof. A Credit Suisse Group company \nmay have acted upon the information and analysis contained in this publication before being made available to clients of Credit Suisse. Investments in emerging \nmarkets are speculative and considerably more volatile than investments in established markets. Some of the main risks are political risks, economic risks, credit \nrisks, currency risks and market risks. Investments in foreign currencies are subject to exchange rate fluctuations. Before entering into any transaction, you \nshould consider the suitability of the transaction to your particular circumstances and independently review (with your professional advisers as necessary) the \nspecific financial risks as well as legal, regulatory, credit, tax and accounting consequences. This document is issued and distributed in the United States by \nCredit Suisse Securities (USA) LLC, a U.S. registered broker-dealer; in Canada by Credit Suisse Securities (Canada), Inc.; and in Brazil by Banco de \nInvestimentos Credit Suisse (Brasil) S.A.  \nThis document is distributed in Switzerland by Credit Suisse AG, a Swiss bank. Credit Suisse is authorized and regulated by the Swiss Financial Market \nSupervisory Authority (FINMA). This document is issued and distributed in Europe (except Switzerland) by Credit Suisse (UK) Limited and Credit Suisse Securities \n(Europe) Limited, London. Credit Suisse Securities (Europe) Limited, London and Credit Suisse (UK) Limited, authorised by the Prudential Regulation Authority \n(PRA) and regulated by the Financial Conduct Authority (FCA) and PRA, are associated but independent legal and regulated entities within Credit Suisse. The \nprotections made available by the UK\u2018s Financial Services Authority for private customers do not apply to investments or services provided by a person outside the \nUK, nor will the Financial Services Compensation Scheme be available if the issuer of the investment fails to meet its obligations. This document is distributed in \nGuernsey by Credit Suisse (Guernsey) Limited, an independent legal entity registered in Guernsey under 15197, with its registered address at Helvetia Court, \nLes Echelons, South Esplanade, St Peter Port, Guernsey. Credit Suisse (Guernsey) Limited is wholly owned by Credit Suisse and is regulated by the Guernsey \nFinancial Services Commission. Copies of the latest audited accounts are available on request. This document is distributed in Jersey by Credit Suisse (Guernsey) \nLimited, Jersey Branch, which is regulated by the Jersey Financial Services Commission. The business address of Credit Suisse (Guernsey) Limited, Jersey \nBranch, in Jersey is: TradeWind House, 22 Esplanade, St Helier, Jersey JE2 3QA. This document has been issued in Asia-Pacific by whichever of the following \nis the appropriately authorised entity of the relevant jurisdiction: in Hong Kong by Credit Suisse (Hong Kong) Limited, a corporation licensed with the Hong Kong \nSecurities and Futures Commission or Credit Suisse Hong Kong branch, an Authorized Institution regulated by the Hong Kong Monetary Authority and a \nRegistered Institution regulated by the Securities and Futures Ordinance (Chapter 571 of the Laws of Hong Kong); in Japan by Credit Suisse Securities (Japan) \nLimited; elsewhere in Asia/Pacific by whichever of the following is the appropriately authorized entity in the relevant jurisdiction: Credit Suisse Equities (Australia) \nLimited, Credit Suisse Securities (Thailand) Limited, Credit Suisse Securities (Malaysia) Sdn Bhd, Credit Suisse AG, Singapore Branch, and elsewhere in the \nworld by the relevant authorized affiliate of the above.  \nThis document may not be reproduced either in whole, or in part, without the written permission of the authors and CREDIT SUISSE.  \n\u00a9 2015 CREDIT SUISSE GROUP AG and/or its affiliates. All rights reserved \n \n \n"
    }
  ]
}